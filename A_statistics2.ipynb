{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCEaMAa+cbT21tWos7S2X7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guneeshkatyal/Statistics-Assignment/blob/main/A_statistics2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROgezociJo_J"
      },
      "outputs": [],
      "source": [
        "\n",
        "## 1. What is hypothesis testing in statistics?\n",
        "Hypothesis testing is a statistical method used to make decisions about a population based on sample data. It involves testing an assumption (hypothesis) using statistical techniques to determine whether there is enough evidence to reject it.\n",
        "\n",
        "## 2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
        "- The **null hypothesis (H0)** states that there is no effect or no difference in a population parameter.\n",
        "- The **alternative hypothesis (H1)** is the opposite of the null hypothesis and suggests that there is a statistically significant effect or difference.\n",
        "\n",
        "## 3. What is the significance level in hypothesis testing, and why is it important?\n",
        "The significance level (denoted as **alpha, α**) represents the probability of rejecting the null hypothesis when it is actually true. A common value is 0.05, meaning a 5% risk of making a Type I error.\n",
        "\n",
        "## 4. What does a P-value represent in hypothesis testing?\n",
        "The **P-value** is the probability of obtaining the observed results, or more extreme results, if the null hypothesis is true. A smaller P-value indicates stronger evidence against H0.\n",
        "\n",
        "## 5. How do you interpret the P-value in hypothesis testing?\n",
        "- If P-value < α: Reject the null hypothesis (significant result).\n",
        "- If P-value ≥ α: Fail to reject the null hypothesis (insufficient evidence).\n",
        "\n",
        "## 6. What are Type 1 and Type 2 errors in hypothesis testing?\n",
        "- **Type I error**: Rejecting H0 when it is actually true (false positive).\n",
        "- **Type II error**: Failing to reject H0 when it is actually false (false negative).\n",
        "\n",
        "## 7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
        "- **One-tailed test**: Tests for an effect in only one direction (greater or smaller).\n",
        "- **Two-tailed test**: Tests for an effect in both directions (greater or smaller).\n",
        "\n",
        "## 8. What is the Z-test, and when is it used in hypothesis testing?\n",
        "A **Z-test** is used to compare sample and population means when the population variance is known and the sample size is large (n > 30).\n",
        "\n",
        "## 9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
        "Z-score formula:\n",
        "\\[ Z = \\frac{(X - \\mu)}{\\sigma} \\]\n",
        "where X = sample mean, μ = population mean, σ = standard deviation.\n",
        "It measures how many standard deviations an observation is from the mean.\n",
        "\n",
        "## 10. What is the T-distribution, and when should it be used instead of the normal distribution?\n",
        "The **T-distribution** is used instead of the normal distribution when the sample size is small (n < 30) and the population variance is unknown.\n",
        "\n",
        "## 11. What is the difference between a Z-test and a T-test?\n",
        "- **Z-test** is used when the population variance is known and the sample size is large.\n",
        "- **T-test** is used when the population variance is unknown and the sample size is small.\n",
        "\n",
        "## 12. What is the T-test, and how is it used in hypothesis testing?\n",
        "A **T-test** compares sample means to determine if they are significantly different from each other.\n",
        "\n",
        "## 13. What is the relationship between Z-test and T-test in hypothesis testing?\n",
        "Both tests compare means, but the T-test is used when population variance is unknown, while the Z-test is used when it is known.\n",
        "\n",
        "## 14. What is a confidence interval, and how is it used to interpret statistical results?\n",
        "A **confidence interval (CI)** is a range of values within which a population parameter is expected to fall with a certain probability (e.g., 95% CI).\n",
        "\n",
        "## 15. What is the margin of error, and how does it affect the confidence interval?\n",
        "The **margin of error** represents the maximum expected difference between the observed and true population parameter. A larger margin results in a wider confidence interval.\n",
        "\n",
        "## 16. How is Bayes' Theorem used in statistics, and what is its significance?\n",
        "**Bayes' Theorem** updates probabilities based on prior knowledge and new evidence. It is used in Bayesian inference for updating beliefs.\n",
        "\n",
        "## 17. What is the Chi-square distribution, and when is it used?\n",
        "The **Chi-square distribution** is used for hypothesis tests involving categorical data, such as goodness-of-fit and independence tests.\n",
        "\n",
        "## 18. What is the Chi-square goodness-of-fit test, and how is it applied?\n",
        "It determines if an observed categorical data distribution matches an expected distribution.\n",
        "\n",
        "## 19. What is the F-distribution, and when is it used in hypothesis testing?\n",
        "The **F-distribution** is used in tests comparing variances, such as ANOVA and F-tests for variance equality.\n",
        "\n",
        "## 20. What is an ANOVA test, and what are its assumptions?\n",
        "**ANOVA (Analysis of Variance)** tests for differences in means across multiple groups. Assumptions include normality, independence, and equal variance.\n",
        "\n",
        "## 21. What are the different types of ANOVA tests?\n",
        "- **One-way ANOVA**: Tests for mean differences in one factor.\n",
        "- **Two-way ANOVA**: Tests for mean differences in two factors.\n",
        "\n",
        "## 22. What is the F-test, and how does it relate to hypothesis testing?\n",
        "An **F-test** is used to compare the variances of two populations and is crucial in ANOVA.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 1. Python program to perform a Z-test\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "sample = [50, 52, 53, 49, 51]\n",
        "pop_mean = 50\n",
        "pop_std = 2\n",
        "\n",
        "z_score, p_value = stats.ttest_1samp(sample, pop_mean)\n",
        "print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
        "```\n",
        "\n",
        "## 2. Simulate random data and calculate P-value\n",
        "```python\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(50, 10, 100)\n",
        "z_score, p_value = stats.ttest_1samp(data, 50)\n",
        "print(f\"P-value: {p_value}\")\n",
        "```\n",
        "Here's your assignment with all the questions answered in a structured format:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean**\n",
        "\n",
        "### **Solution:**\n",
        "A one-sample Z-test is used when we want to compare the mean of a sample to a known population mean, assuming that the population standard deviation is known.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def one_sample_z_test(sample, pop_mean, pop_std):\n",
        "    sample_mean = np.mean(sample)\n",
        "    n = len(sample)\n",
        "    z_score = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))\n",
        "    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "    return z_score, p_value\n",
        "\n",
        "# Example\n",
        "sample = [50, 52, 53, 48, 47, 51, 49, 52]\n",
        "pop_mean = 50\n",
        "pop_std = 3\n",
        "\n",
        "z_score, p_value = one_sample_z_test(sample, pop_mean, pop_std)\n",
        "print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "- If p-value < significance level (e.g., 0.05), we reject the null hypothesis that the sample mean is equal to the population mean.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot**\n",
        "\n",
        "### **Solution:**\n",
        "A two-tailed Z-test checks if the sample mean is significantly different from the population mean in both directions.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_z_test(alpha=0.05):\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "    y = norm.pdf(x, 0, 1)\n",
        "\n",
        "    critical = norm.ppf(1 - alpha / 2)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.lineplot(x, y)\n",
        "\n",
        "    plt.fill_between(x, y, where=(x <= -critical) | (x >= critical), color='red', alpha=0.5, label=\"Rejection Region\")\n",
        "    plt.fill_between(x, y, where=(x > -critical) & (x < critical), color='blue', alpha=0.5, label=\"Acceptance Region\")\n",
        "\n",
        "    plt.axvline(-critical, linestyle='--', color='black')\n",
        "    plt.axvline(critical, linestyle='--', color='black')\n",
        "    plt.legend()\n",
        "    plt.title(\"Two-Tailed Z-Test Decision Region\")\n",
        "    plt.xlabel(\"Z-score\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.show()\n",
        "\n",
        "plot_z_test()\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "- The red areas represent the rejection regions where we reject the null hypothesis.\n",
        "- If the computed Z-score falls in these regions, the sample mean is significantly different.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing**\n",
        "\n",
        "### **Solution:**\n",
        "Type 1 error occurs when we reject a true null hypothesis, and Type 2 error occurs when we fail to reject a false null hypothesis.\n",
        "\n",
        "```python\n",
        "def plot_type1_type2(mu0=0, mu1=2, sigma=1, alpha=0.05):\n",
        "    x = np.linspace(-4, 6, 1000)\n",
        "    y_H0 = norm.pdf(x, mu0, sigma)\n",
        "    y_H1 = norm.pdf(x, mu1, sigma)\n",
        "\n",
        "    critical_value = norm.ppf(1 - alpha)\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.lineplot(x, y_H0, label=\"H0 (Null Distribution)\")\n",
        "    sns.lineplot(x, y_H1, label=\"H1 (Alternative Distribution)\")\n",
        "\n",
        "    plt.fill_between(x, y_H0, where=(x > critical_value), color='red', alpha=0.5, label=\"Type I Error (α)\")\n",
        "    plt.fill_between(x, y_H1, where=(x < critical_value), color='blue', alpha=0.5, label=\"Type II Error (β)\")\n",
        "\n",
        "    plt.axvline(critical_value, linestyle='--', color='black')\n",
        "    plt.legend()\n",
        "    plt.title(\"Type 1 and Type 2 Errors\")\n",
        "    plt.xlabel(\"Test Statistic\")\n",
        "    plt.ylabel(\"Probability Density\")\n",
        "    plt.show()\n",
        "\n",
        "plot_type1_type2()\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "- The red area represents **Type 1 error** (rejecting a true null hypothesis).\n",
        "- The blue area represents **Type 2 error** (failing to reject a false null hypothesis).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Write a Python program to perform an independent T-test and interpret the results**\n",
        "\n",
        "### **Solution:**\n",
        "An independent T-test compares the means of two independent groups.\n",
        "\n",
        "```python\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "group1 = [50, 52, 53, 48, 47, 51, 49, 52]\n",
        "group2 = [55, 57, 58, 54, 53, 56, 55, 58]\n",
        "\n",
        "t_stat, p_value = ttest_ind(group1, group2)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "- If p-value < 0.05, we reject the null hypothesis that the two groups have the same mean.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Perform a paired sample T-test using Python and visualize the comparison results**\n",
        "\n",
        "### **Solution:**\n",
        "A paired T-test is used for dependent samples (e.g., before and after measurements).\n",
        "\n",
        "```python\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "before = [50, 52, 53, 48, 47, 51, 49, 52]\n",
        "after = [55, 57, 58, 54, 53, 56, 55, 58]\n",
        "\n",
        "t_stat, p_value = ttest_rel(before, after)\n",
        "print(f\"Paired T-test: T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "- If p-value < 0.05, the treatment had a significant effect.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Simulate data and perform both Z-test and T-test, then compare the results using Python**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=50, scale=5, size=30)\n",
        "pop_mean = 50\n",
        "pop_std = 5\n",
        "\n",
        "z_stat = (np.mean(sample) - pop_mean) / (pop_std / np.sqrt(len(sample)))\n",
        "z_p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
        "\n",
        "t_stat, t_p_value = stats.ttest_1samp(sample, pop_mean)\n",
        "\n",
        "print(f\"Z-test: Z-stat={z_stat}, p-value={z_p_value}\")\n",
        "print(f\"T-test: T-stat={t_stat}, p-value={t_p_value}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.**\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "\n",
        "def confidence_interval(sample, confidence=0.95):\n",
        "    mean = np.mean(sample)\n",
        "    sem = stats.sem(sample)\n",
        "    margin = sem * stats.t.ppf((1 + confidence) / 2., len(sample)-1)\n",
        "\n",
        "    return mean - margin, mean + margin\n",
        "\n",
        "sample = np.random.normal(50, 5, 30)\n",
        "ci = confidence_interval(sample)\n",
        "print(f\"95% Confidence Interval: {ci}\")\n",
        "```\n",
        "\n",
        "# 10. Calculate margin of error for a given confidence level\n",
        "def margin_of_error(sample, confidence=0.95):\n",
        "    sem = stats.sem(sample)\n",
        "    margin = sem * stats.t.ppf((1 + confidence) / 2., len(sample)-1)\n",
        "    return margin\n",
        "\n",
        "sample = np.random.normal(50, 5, 30)\n",
        "print(\"Margin of Error:\", margin_of_error(sample))\n",
        "\n",
        "# 11. Implement Bayesian inference using Bayes' Theorem\n",
        "def bayes_theorem(prior, likelihood, evidence):\n",
        "    posterior = (likelihood * prior) / evidence\n",
        "    return posterior\n",
        "\n",
        "prior = 0.5\n",
        "likelihood = 0.8\n",
        "evidence = 0.6\n",
        "print(\"Posterior Probability:\", bayes_theorem(prior, likelihood, evidence))\n",
        "\n",
        "# 12. Chi-square test for independence\n",
        "def chi_square_test(data):\n",
        "    chi2, p, dof, expected = stats.chi2_contingency(data)\n",
        "    return chi2, p\n",
        "\n",
        "data = np.array([[10, 20, 30], [6, 9, 17]])\n",
        "print(\"Chi-Square Test:\", chi_square_test(data))\n",
        "\n",
        "# 13. Calculate expected frequencies for Chi-square test\n",
        "def expected_frequencies(data):\n",
        "    _, _, _, expected = stats.chi2_contingency(data)\n",
        "    return expected\n",
        "\n",
        "print(\"Expected Frequencies:\", expected_frequencies(data))\n",
        "\n",
        "# 14. Goodness-of-fit test\n",
        "def goodness_of_fit_test(observed, expected):\n",
        "    chi2, p = stats.chisquare(observed, expected)\n",
        "    return chi2, p\n",
        "\n",
        "observed = np.array([50, 30, 20])\n",
        "expected = np.array([40, 40, 20])\n",
        "print(\"Goodness of Fit Test:\", goodness_of_fit_test(observed, expected))\n",
        "\n",
        "# 15. Visualize Chi-square distribution\n",
        "def plot_chi_square(df):\n",
        "    x = np.linspace(0, 10, 100)\n",
        "    y = stats.chi2.pdf(x, df)\n",
        "    plt.plot(x, y, label=f'df={df}')\n",
        "    plt.title(\"Chi-Square Distribution\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_chi_square(3)\n",
        "\n",
        "# 16. F-test for variance comparison\n",
        "def f_test(sample1, sample2):\n",
        "    f_stat = np.var(sample1, ddof=1) / np.var(sample2, ddof=1)\n",
        "    p_value = 1 - stats.f.cdf(f_stat, len(sample1)-1, len(sample2)-1)\n",
        "    return f_stat, p_value\n",
        "\n",
        "sample1 = np.random.normal(50, 5, 30)\n",
        "sample2 = np.random.normal(55, 7, 30)\n",
        "print(\"F-Test:\", f_test(sample1, sample2))\n",
        "\n",
        "# 17. ANOVA test\n",
        "def anova_test(*groups):\n",
        "    f_stat, p_value = stats.f_oneway(*groups)\n",
        "    return f_stat, p_value\n",
        "\n",
        "group1 = np.random.normal(50, 5, 30)\n",
        "group2 = np.random.normal(52, 5, 30)\n",
        "group3 = np.random.normal(48, 5, 30)\n",
        "print(\"ANOVA Test:\", anova_test(group1, group2, group3))\n",
        "\n",
        "# 18. One-way ANOVA with plot\n",
        "def plot_oneway_anova(groups):\n",
        "    plt.boxplot(groups, labels=[\"Group1\", \"Group2\", \"Group3\"])\n",
        "    plt.title(\"One-way ANOVA\")\n",
        "    plt.show()\n",
        "\n",
        "plot_oneway_anova([group1, group2, group3])\n",
        "\n",
        "# 19. Check ANOVA assumptions\n",
        "def check_anova_assumptions(*groups):\n",
        "    normality = [stats.shapiro(group)[1] > 0.05 for group in groups]\n",
        "    homogeneity = stats.levene(*groups)[1] > 0.05\n",
        "    return normality, homogeneity\n",
        "\n",
        "print(\"ANOVA Assumptions:\", check_anova_assumptions(group1, group2, group3))\n",
        "\n",
        "# 20. Two-way ANOVA (using statsmodels)\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"Factor1\": np.repeat([\"A\", \"B\"], 30),\n",
        "    \"Factor2\": np.tile([\"X\", \"Y\"], 30),\n",
        "    \"Value\": np.random.normal(50, 5, 60)\n",
        "})\n",
        "\n",
        "model = smf.ols(\"Value ~ Factor1 + Factor2 + Factor1:Factor2\", data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model)\n",
        "print(\"Two-way ANOVA:\")\n",
        "print(anova_table)\n",
        "\n",
        "# 21. Visualize F-distribution\n",
        "def plot_f_distribution(df1, df2):\n",
        "    x = np.linspace(0, 5, 1000)\n",
        "    y = stats.f.pdf(x, df1, df2)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(\"F-Distribution\")\n",
        "    plt.show()\n",
        "\n",
        "plot_f_distribution(5, 10)\n",
        "\n",
        "# 22. One-way ANOVA with boxplots\n",
        "def plot_anova_boxplot(*groups):\n",
        "    plt.boxplot(groups, labels=[f'Group {i+1}' for i in range(len(groups))])\n",
        "    plt.title(\"One-way ANOVA Boxplot\")\n",
        "    plt.show()\n",
        "\n",
        "plot_anova_boxplot(group1, group2, group3)\n",
        "\n",
        "# 23. Simulate normal data and perform hypothesis testing\n",
        "simulated_data = np.random.normal(50, 5, 100)\n",
        "t_stat, p_value = stats.ttest_1samp(simulated_data, 50)\n",
        "print(\"T-test on simulated data:\", t_stat, p_value)\n",
        "\n",
        "# 24. Chi-square test for population variance\n",
        "def chi_square_variance_test(sample, variance):\n",
        "    chi2 = (len(sample)-1) * np.var(sample, ddof=1) / variance\n",
        "    p_value = 1 - stats.chi2.cdf(chi2, len(sample)-1)\n",
        "    return chi2, p_value\n",
        "\n",
        "print(\"Chi-square Variance Test:\", chi_square_variance_test(sample1, 25))\n",
        "\n",
        "# 25. Z-test for proportions\n",
        "def z_test_proportions(p1, n1, p2, n2):\n",
        "    p_combined = (p1 * n1 + p2 * n2) / (n1 + n2)\n",
        "    se = np.sqrt(p_combined * (1 - p_combined) * (1/n1 + 1/n2))\n",
        "    z = (p1 - p2) / se\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p_value\n",
        "\n",
        "print(\"Z-test for proportions:\", z_test_proportions(0.4, 100, 0.5, 100))\n",
        "\n",
        "# 26. F-test for variances with visualization\n",
        "plot_anova_boxplot(sample1, sample2)\n",
        "\n",
        "# 27. Chi-square goodness of fit test\n",
        "print(\"Chi-square Goodness of Fit:\", goodness_of_fit_test(observed, expected))\n",
        "\n"
      ],
      "metadata": {
        "id": "pdyVSmeOLui5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}